#!/usr/bin/env python3
"""
Build list for evaluating keyword search.
Requires `text_preproc` to be run first!

This script constructs the KEYWORD_LIST JSON file, which contains
information about keywords used for the Interspeech 2026 KWS experiment.
See `build_keyword_list` for JSON data structure.
"""

import os
import pandas as pd
from tqdm import tqdm
import random
import json
from argparse import ArgumentParser, Namespace
from typing import Dict, List, Union
from unidecode import unidecode

# local imports
from src.constants import (
    WORDS_CSV, WORD2PHRASE_PATH, KEYWORD_LIST
)


def build_keyword_list(
        word_df: pd.DataFrame,
        word2phrase: List[List[int]],
        token_count: int=10,
        num_keywords: int=30,
        min_keyword_length: int=5,
        random_seed: int=1337,
    ) -> List[Dict[str, Union[str, List[int]]]]:
    """
    Samples a set of keywords from the word DataFrame with specified token
    count and number of keywords, and maps keywords to audio records from
    the Tira ASR dataset. Returns a list of keyword dicts with the following
    structure:
    ```json
    [
        {
            'keyword': $str,
            'keyword_idcs': [$int, $int, ... ], # added by `make_keyword_ds.py`
            'keyword_record_idcs': [$int, $int, ...]
            'positive_record_idcs': [$int, $int, ...]
        },
        ...
    ]
    ```
    - The 'keyword' key gives the string of the keyword
    - The 'keyword_record_idcs' key maps to the indices of records that query
        recordings are sampled from.
    - The 'positive_record_idcs' key maps to the indices of records that contain the
        keyword (i.e., positive examples) used as targets (rather than queries)
        during KWS evaluation.
    - The key 'keyword_idcs' maps to the indices of all query tokens for the given
        keyword in the keyword dataset (built using `make_keyword_ds.py`). This key
        is NOT built in this function but is added later.
    

    Args:
        word_df: DataFrame with unique phrases and token counts
        word2phrase: Mapping from words to phrases containing the word
        token_count: Number of tokens for each keyword.
        num_keywords: Number of keywords to select.
        min_keyword_length: Minimum length of each keyword in characters.
        random_seed: Random seed for reproducibility.
    Returns:
        keyword_list: List of keyword dicts
    """
    # multiply token count by 2 to account for sampling both
    # queries and positive targets
    token_mask = word_df['token_count'] >= token_count*2
    print(f" Words with >= {token_count*2} tokens: {token_mask.sum()}")

    # apply unidecode so that combining diacritics don't affect length
    unidecode_words = word_df['word'].apply(unidecode)
    length_mask = unidecode_words.str.len() >= min_keyword_length
    print(f" Words with length >= {min_keyword_length}: {length_mask.sum()}")

    candidate_mask = token_mask & length_mask
    print(f" Candidate words: {candidate_mask.sum()}")
    print(f" Sampling {num_keywords} keywords...")

    keyword_df = word_df[candidate_mask].sample(
        n=num_keywords,
        random_state=random_seed,
    )
    keyword_list = []

    for index, word in keyword_df['word'].items(): # type: ignore
        index: int
        phrase_idcs = word2phrase[index]

        # split phrases into query and positive sets
        random.Random(random_seed).shuffle(phrase_idcs)
        phrase_idcs = phrase_idcs[:token_count*2]
        split_idx = len(phrase_idcs) // 2
        query_phrase_idcs = phrase_idcs[:split_idx]
        positive_phrase_idcs = phrase_idcs[split_idx:]

        keyword_list.append({
            'keyword': word,
            'keyword_record_idcs': query_phrase_idcs,
            'positive_record_idcs': positive_phrase_idcs,
        })

    return keyword_list

def main():
    args = get_args()

    # Load files generated by `text_preproc.py`
    word_df = pd.read_csv(WORDS_CSV)
    print(f"Loaded {len(word_df)} unique words from {WORDS_CSV}")

    with open(WORD2PHRASE_PATH, 'r', encoding='utf8') as f:
        word2phrase = [list(map(int, line.strip().split())) for line in f.readlines()]
    print(f"Loaded word2phrase mapping from {WORD2PHRASE_PATH}")

    print(f"Building keyword list with token_count={args.token_count} and num_keywords={args.num_keywords}...")

    # Build keyword list
    keyword_list = build_keyword_list(
        word_df,
        word2phrase,
        token_count=args.token_count,
        num_keywords=args.num_keywords,
        min_keyword_length=args.min_keyword_length,
        random_seed=args.random_seed,
    )

    print(f"Saving keyword list with {len(keyword_list)} keywords to {KEYWORD_LIST}...")
    with open(KEYWORD_LIST, 'w', encoding='utf8') as f:
        json.dump(keyword_list, f, indent=4, ensure_ascii=False)

    print("\nâœ“ Keyword list built successfully!")


def get_args() -> Namespace:
    parser = ArgumentParser(description='Build KWS lists and associated files')
    parser.add_argument(
        '--token_count', type=int, default=10,
        help='Number of tokens to be used for keywords.'
    )
    parser.add_argument(
        '--num_keywords', type=int, default=30,
    )
    parser.add_argument(
        '--min_keyword_length', type=int, default=5,
        help='Minimum length of each keyword in characters.'
    )
    parser.add_argument(
        '--random_seed', type=int, default=1337,
        help='Random seed for reproducibility'
    )
    args = parser.parse_args()
    return args


if __name__ == '__main__':
    main()

