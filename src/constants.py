from importlib.util import find_spec
import torch
import os
from pathlib import Path

# audio and model constants

## hyperparameters

SAMPLE_RATE = 16_000
DEVICE = torch.device(0 if torch.cuda.is_available() else "cpu")
BATCH_SIZE = int(os.environ.get("BATCH_SIZE", 256))

## model names

CLAP_IPA_ENCODER_NAME = 'anyspeech/clap-ipa-{encoder_size}-speech'
CLAP_IPA_TEXT_ENCODER_NAME = 'anyspeech/clap-ipa-{encoder_size}-phone'
IPA_ALIGNER_ENCODER_NAME = 'anyspeech/ipa-align-{encoder_size}-speech'

SPEECHBRAIN_LID_ENCODER_NAME = 'speechbrain/lang-id-voxlingua107-ecapa'

# file paths

## dataset paths
DATASETS_DIR = os.environ.get("DATASETS", "~/datasets")
DATASETS_DIR = Path(os.path.expanduser(DATASETS_DIR))
TIRA_ASR_PATH = DATASETS_DIR / "tira_asr"
TIRA_DRZ_PATH = DATASETS_DIR / "tira_drz"
TIRA_ASR_URI = "css-kws-capstone/tira-asr"

TIRA_ELICITATION = DATASETS_DIR / "tira_elicitation"
SUPERVISION_MANIFEST = TIRA_ELICITATION / "supervision_manifest.jsonl"
RECORDING_MANIFEST = TIRA_ELICITATION / "recording_manifest.jsonl"

## local data paths
PROJECT_DIR = Path(__file__).parent.parent
DATA_DIR = PROJECT_DIR / "data"
LABELS_DIR = DATA_DIR / "labels"
CONFIG_DIR = PROJECT_DIR / "config"

"""
## Keyword/phrase source data
Files related to words and phrases extracted from the Tira ASR dataset.
See `scripts/data_builders/text_preproc.py` for details.
"""

# directories for word and phrase data
WORDS_DIR =             LABELS_DIR / "words"
PHRASES_DIR =           LABELS_DIR / "phrases"

# dataframe mapping `eaf_text` (raw string from ELAN)
# to `fst_text` (string with most likely normalized form using FST parser).
MERGED_PHRASES_CSV =    PHRASES_DIR / "keyphrases_rewritten_merges.csv"

# dataframe where each row is a unique phrase (only
# uses FST-normalized text) along with its token count
# gloss and lemmata.
PHRASES_CSV =           PHRASES_DIR / "tira_phrases.csv"

# matrix of character error rates (CER) between
# all phrases in `PHRASES_CSV`.
# Row i, column j corresponds to CER between
# phrase i and phrase j.
CER_MATRIX_PATH =       PHRASES_DIR / "cer_matrix.np"

# CSV mapping Tira record indices to phrase indices.
# Each row has format: `record_idx, phrase_idx`
PHRASE2RECORDS_PATH =   PHRASES_DIR / "record2phrase.csv"

# similar to `PHRASES_CSV`, each row is a unique word
# (FST-normalized) along with its token count and lemma form.
WORDS_CSV =             WORDS_DIR / "tira_words.csv"

# CSV mapping Tira word indices to phrase containing them
# Each row has format: `word_idx, phrase_idx`
# Note this is a many-to-many relation
WORD2PHRASE_PATH =      WORDS_DIR / "word2phrase.csv"

"""
### Keyword lists
Lists related to keyword queries for the Interspeech 2026 experiment.
Generated by `scripts/data_builders/keyphrase_list_builder.py`
"""

KEYWORDS_DIR =          LABELS_DIR / "keywords"

# JSON list indicating keywords, positive and negative records, etc.
KEYWORD_LIST =          KEYWORDS_DIR / "keyword_list.json"

# CSV, subset of `WORDS_CSV` containing only the words used as keywords
# for the Interspeech 2026 experiment.
KEYWORDS_CSV =          KEYWORDS_DIR / "keywords.csv"

# CSV indicating Tira ASR records used for positive and negative phrases
KEYWORD_SENTENCES =     KEYWORDS_DIR / "keyword_sentence_ids.csv"

"""
### MFA data
Files needed for forced alignment using Montreal Forced Aligner (MFA).
"""

# directory for storing MFA-related files
ALIGNMENT_DIR =         LABELS_DIR / "alignment"

# TXT file containing MFA-format pronunciation dictionary
# for all words in `WORDS_CSV`. Each line has format `word\tpronunciation`.
MFA_DICT_PATH =         ALIGNMENT_DIR / "mfa_dict.txt"

# Directory containing audio corpus conforming to MFA format.
# Stored in `DATASETS_DIR` rather than within the project
MFA_CORPUS_DIR =        DATASETS_DIR / "tira_mfa"

# Directory for storing MFA output TextGrids
MFA_OUTPUT_DIR =        ALIGNMENT_DIR / "mfa_output"


"""
### Keyphrase lists
Lists related to keyphrases used for the Sparse Annotation Filling experiment.
"""

# similar to `PHRASES_CSV` but also including columns
# for which phrases are used as keyphrase queries and how many
# easy/medium/hard negative records exist for a given phrase.
KEYPHRASE_CSV =             PHRASES_DIR / "keyphrases.csv"

# JSON list for all keyphrases including positive
# and negative records.
KEYPHRASE_LIST =            PHRASES_DIR / "keyphrase_list.json"

# JSON list for calibration keyphrases, same format as KEYPHRASE_LIST
CALIBRATION_LIST =          PHRASES_DIR / "calibration_list.json"

# TXT list for English calibration keyphrase indices
ENGLISH_CALIBRATION_LIST =  PHRASES_DIR / "english_calibration_keyphrase_idcs.txt"

"""
### output files
Files related to outputs from keyword search experiments.
"""

# folder for storing predicted outputs from KWS
KWS_PREDICTIONS = DATA_DIR / "kws_predictions"

# folder for storing similarity matrices for embeddings used in KWS
SIMILARITY_MATRICES = DATA_DIR / "similarity_matrix/"

# folder for storing embeddings used for KWS
EMBEDDINGS = DATA_DIR / "tira_kws" / "embeddings"

# model and inference constants
WFST_BATCH_SIZE = 1024
MAX_KEYWORD_STR = '$max'
MEAN_KEYWORD_STR = '$mean'
CALIBRATION_NUM_NEGATIVE = 50
CALIBRATION_NUM_POSITIVE = 10
CLAP_IS_AVAILABLE = find_spec('clap') is not None
SPEECHBRAIN_IS_AVAILABLE = find_spec('speechbrain') is not None
WAV2VEC_DOWNSAMPLE_FACTOR = 320
